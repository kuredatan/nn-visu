"Interpretation of Neural Networks"

In the proposal, we have written:

"+ How does the training NN "integrates" the new input image in the images extracted from filters? + An idea to quantify the "amount of the input image used to train the filters" is as follows: we will focus on one object class and one layer of the network, keep the original filter image extracted before training, train the network with a batch of 64-128 images from the considered class, extract SIFT descriptors from all images (including "filter" images), and perform a Bag-of-Words analysis: extracting visual (by performing K-means on SIFT descriptors), compute histograms from the SIFT descriptor of each image, and compute a distance score between histograms to see how each image influenced the filter. One can also check how correspondence point matching performs when applied to filter image and one of the input images. The null hypothesis is that the filters of the network are (once "deconvoluted") a superposition of increasing (with respect to the filter depth) higher-order features from every training image. The training aims then at  fine-tuning the parameters for the feature extraction for each filter."

As advised by our supervisor, we have only focused on this part of the proposal, in order to answer the question:

"+ How does the training NN "integrates" the new input image in the images extracted from filters? +"

We focus on the class of Siamese Cats (class 284 in ImageNet labels) and we have got the images from Google Images (there might be overlaps with CIFAR-10 and ImageNet). There are 22 images on it. When the batch size is bigger than the number of images, we repeat the images.

Code is available at https://github.com/kuredatan/nn-visu.

We have chosen VGG (REF) and our own model "Conv" (see file conv_training_acc_loss.csv for the traces for the training of Conv, and models.py/architecture.png for the architecture) for our experiments, with the weights provided by Keras trained on ImageNet for the former and the trained weights on CIFAR-10.

Easily create NN architecture : http://alexlenail.me/NN-SVG/LeNet.html
Credit for the "Conv" architecture: https://blog.plon.io/tutorials/cifar-10-classification-using-keras-tutorial/

* Explain what the BoW analysis is
* Explain what the SIFT analysis is
* Explain what the Harris analysis is

We will test the following pipeline for every convolutional or pooling layer in the model:
1) Let us denote "layer" the considered layer
2) Save the filter images associated with the layer (a few of the feature maps, for instance, n=5, when relevant) and deconvolve them
3) Train the network with a batch of 32 images from the considered class
4) Save the filter images associated with the layer (a few of the feature maps, for instance, n=5, when relevant, the same as in (2)) and deconvolve them

Another pipeline to see the evolution of the detection of corner, SIFT descriptors, etc.
1) Get the outputs for each layer of the two models for one image from the dataset CATS
2) Perform BoW analysis on the deconvoluted filter images and the original images from the class
3) Perform SIFT analysis on the deconvoluted filter images and the original images from the class
4) Perform Harris analysis on the deconvoluted filter images and the original images from the class
5) Compare the values obtained and try to interpret them
Values for layer=conv1 and model=conv for BOW: bow_scores.dat
for SIFT: contributions.dat
for Harris: contributions.dat
Called with "python3.6 analysis_fmaps.py --tname conv/convfeature_map_layer_conv1.png --tmethod {harris,bow,sift}"

